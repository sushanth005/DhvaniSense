ğŸ§ DhvaniSense - AI Voice Detector (Human vs AI Audio Classification API)

DhvaniSense is an AI-powered FastAPI backend project that detects whether a given audio sample is **AI-generated** or **Human-generated**.  
It supports `.mp3` audio inputs (and `.wav` optionally) and provides a production-ready API endpoint for real-time classification.

This project includes:
âœ… Dataset checking  
âœ… Training pipeline  
âœ… Validation pipeline  
âœ… Report generation  
âœ… FastAPI server deployment  
âœ… API + Client testing scripts  

---

# ğŸš€ Features

- ğŸ™ï¸ Detect AI-generated vs Human voice from audio files
- âš¡ FastAPI backend with clean API endpoints
- ğŸ” API Key authentication support
- ğŸ§  Model training + checkpoint saving
- ğŸ“Š Validation results exported into CSV
- ğŸ“„ Report generation supported
- ğŸ³ Docker support for deployment
- â˜ï¸ Cloud deploy-ready (Google Cloud / Azure / AWS)
- ğŸ“¥ Supports Base64 encoded audio request payload

---

# ğŸ—ï¸ Project Workflow (Pipeline)

Follow the pipeline in this exact order:

requirements.txt
â†“
test_paths.py
â†“
app/check_data.py
â†“
scripts/train_mini.py (takes time, creates models)
â†“
scripts/validate.py
â†“
scripts/generate_report.py
â†“
uvicorn app.main:app
â†“
app/test_api.py
â†“
test_client.py


---

# ğŸ“ Folder Structure

DhvaniSense/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ pycache/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ auth.py
â”‚ â”œâ”€â”€ check_data.py
â”‚ â”œâ”€â”€ engine.py
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ test_api.py
â”‚ â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ ai/
â”‚ â””â”€â”€ human/
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ checkpoints/
â”‚ â”œâ”€â”€ final_voice_model/
â”‚ â””â”€â”€ config.json
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ generate_report.py
â”‚ â”œâ”€â”€ train_mini.py
â”‚ â””â”€â”€ validate.py
â”‚
â”œâ”€â”€ venv/ # Local virtual environment (not for GitHub)
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_client.py
â”œâ”€â”€ test_paths.py
â””â”€â”€ validation_results.csv


---

# âš™ï¸ Requirements

- Python **3.10+**
- pip installed
- FFmpeg installed (**required for `.mp3` support**)

---

# ğŸ”§ Installation Setup

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/<your-username>/DhvaniSense.git
cd DhvaniSense
2ï¸âƒ£ Create Virtual Environment
python -m venv venv
Activate it:

Windows
venv\Scripts\activate
Linux / Mac
source venv/bin/activate
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
ğŸµ MP3 Support (FFmpeg Required)
Since this project supports .mp3 audio, FFmpeg must be installed.

âœ… Check FFmpeg Installation
ffmpeg -version
If it shows version details, you're good.

ğŸªŸ Install FFmpeg on Windows
Download FFmpeg from: https://ffmpeg.org/download.html

Extract the zip file

Add the bin/ folder to Windows Environment Variables (PATH)

Example:

C:\ffmpeg\bin
Restart terminal after adding PATH.

ğŸ“Œ Dataset Setup
Place your dataset in this exact format:

data/
  â”œâ”€â”€ ai/
  â”‚     â”œâ”€â”€ sample1.mp3
  â”‚     â”œâ”€â”€ sample2.mp3
  â”‚     â””â”€â”€ ...
  â””â”€â”€ human/
        â”œâ”€â”€ sample1.mp3
        â”œâ”€â”€ sample2.mp3
        â””â”€â”€ ...
Supported formats:

.mp3 âœ…

.wav âœ…

âœ… Step-by-Step Execution (Correct Order)
âœ… Step 1: Verify Paths
Run:

python test_paths.py
This ensures:

all directories exist

dataset paths are correct

models/scripts folders are present

âœ… Step 2: Check Dataset
Run:

python app/check_data.py
This checks:

dataset folder validity

corrupted/unreadable audio files

missing audio samples

âœ… Step 3: Train Model (Main Training Script)
Run:

python scripts/train_mini.py
âš ï¸ This step takes time depending on dataset size and system speed.

This generates model outputs inside:

models/final_voice_model/
models/checkpoints/
models/config.json
âœ… Step 4: Validate Model
Run:

python scripts/validate.py
This generates:

validation_results.csv
âœ… Step 5: Generate Report
Run:

python scripts/generate_report.py
This script generates a final evaluation report from validation results.

ğŸŒ Run the FastAPI Server
Start the backend server using uvicorn:

uvicorn app.main:app --host 0.0.0.0 --port 8000
Server runs at:

http://127.0.0.1:8000
ğŸ“‘ API Documentation
Once the server is running, open:

Swagger UI
http://127.0.0.1:8000/docs
Redoc
http://127.0.0.1:8000/redoc
ğŸ”‘ Authentication (API Key Support)
API key authentication logic is implemented in:

app/auth.py
Requests must include the header:

x-api-key: <YOUR_API_KEY>
ğŸ“Œ API Endpoints
âœ… Health Check
GET

/health
Example response:

{
  "status": "ok"
}
ğŸ§ Detect AI vs Human Voice (Base64 Input)
POST

/api/v1/detect
This endpoint accepts Base64 encoded audio input.

ğŸ“¥ Request Body
{
  "language": "Tamil",
  "audioFormat": "mp3",
  "audioBase64": "BASE64_ENCODED_AUDIO"
}
ğŸ“¤ Success Response
{
  "status": "success",
  "language": "Tamil",
  "classification": "AI_GENERATED",
  "confidenceScore": 0.87,
  "explanation": "Low jitter and low shimmer indicate synthetic voice patterns"
}
âŒ Error Response
{
  "status": "error",
  "message": "Invalid API key"
}
ğŸ§ª Testing the API
âœ… Step 1: Run API Test Script
Start the server first:

uvicorn app.main:app --host 0.0.0.0 --port 8000
Then run:

python app/test_api.py
âœ… Step 2: Run Client Test Script
python test_client.py
This behaves like an external user/client.

ğŸ“Œ Example Curl Request
curl -X POST "http://127.0.0.1:8000/api/v1/detect" \
-H "Content-Type: application/json" \
-H "x-api-key: your_api_key_here" \
-d '{
  "language": "Tamil",
  "audioFormat": "mp3",
  "audioBase64": "BASE64_ENCODED_AUDIO"
}'
ğŸ³ Docker Deployment
Build Docker Image
docker build -t dhvanisense .
Run Docker Container
docker run -p 8000:8000 dhvanisense
Now open:

http://127.0.0.1:8000/docs
â˜ï¸ Cloud Deployment
DhvaniSense can be deployed on:

Google Cloud Run

Google Compute Engine VM

Azure VM

AWS EC2

After deployment, your API endpoint will look like:

http://<public-ip>/api/v1/detect
ğŸ“Š Output Files Generated
After running the full pipeline, these outputs will exist:

models/final_voice_model/ â†’ Final trained model

models/checkpoints/ â†’ Training checkpoints

models/config.json â†’ Model configuration file

validation_results.csv â†’ Validation results

ğŸ› ï¸ Common Errors & Fixes
âŒ Error: FFmpeg not found / mp3 not loading
âœ… Fix: Install FFmpeg and ensure it is in PATH.

Check:

ffmpeg -version
âŒ Error: Model not found
This happens if you did not train the model.

âœ… Fix:

python scripts/train_mini.py
âŒ Error: Dataset missing
Ensure folder structure is correct:

data/ai/
data/human/
âŒ Error: Uvicorn not installed
Install it:

pip install uvicorn
ğŸ§  Tech Stack
Python

FastAPI

Uvicorn

NumPy

Pandas

Librosa (Audio Feature Extraction)

Scikit-learn / Deep Learning (based on your model)

FFmpeg (MP3 decoding)

Docker

ğŸ† Use Cases
Deepfake voice detection

Voice authenticity verification

AI voice fraud detection

Hackathon security solutions

Audio classification research

ğŸ‘¨â€ğŸ’» Author
Developed by Sushanth Bandari
Project Name: DhvaniSense

ğŸ“œ License
This project is intended for educational and hackathon purposes.
You may add an MIT / Apache license depending on your requirement.

â­ Future Improvements
Add frontend UI for audio upload and detection

Improve dataset scaling and augmentation

Add GPU inference support

Add database logging for API requests

Add HTTPS + Domain integration for production deployment

Add support for more languages (Hindi, Telugu, Kannada, etc.)

âœ… Final Note
Run the pipeline in order:

âœ… Paths â†’ Data Check â†’ Train â†’ Validate â†’ Report â†’ Run API â†’ Test API

Then DhvaniSense will work smoothly ğŸš€

